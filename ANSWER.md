### Как бы вы тестировали/проверяли корректность, полноту и неизбыточность ваших тестов?

Для проверки корректности, я бы прошёл по заданию, составил тест-кейсы и сравнил бы их с тестами, написанными студентом

Для проверки на полноту, я бы внёс ошибки в тестируемую программу в каждую её ветвь исполнения, и проверил,
что каждый из тестов упал. Посмотрел, что конкретно тестирует каждый тест, проверены ли все граничные значения,
присутствуют ли проверки негативных сценариев, покрыты ли все классы эквивалентности и состояния-переходы,
не выполняются ли повторные проверки в рамках одного и того же класса эквивалентности или состояния-перехода.

### Как бы вы осуществляли эти проверки в автоматическом режиме?

Если студенты тестируют готовую программу или пишут интеграционные тесты
(неважно, по принципу "белого" или "чёрного" ящика), можно было бы выполнять проверки,
что написанные студентом тесты падают при намеренном внесении ошибки в логику программы.
Таким образом, можно было бы запускать студенческие тесты автоматически на нескольких вариантах программы -
рабочем и с ошибками

Если говорить о юнит-тестировании, можно было бы проверять с помощью spy-патчинга исходной программы,
даже если она написана студентом, проверять, с какими аргументами во время запуска тестов вызывается тестируемый код,
и смотреть, чтобы не было несколько вызовов в рамках одного класса эквивалентности,
и обязательно были покрыты все классы эквивалентности и эдж-кейсы

Если говорить о функциональном тестировании, можно проверять также как и при юнит-тестировании,
но следить за цепочками вызовов и проверять на наличие обязательных наборов шагов пользовательских сценариев,
как позитивных, так и негативных, проверять, что все сценарии покрыты и не делаются лишние проверки, нет дублей
